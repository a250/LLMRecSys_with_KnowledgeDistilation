# Experiment 0. Toy example
#   dataset: MoveLeanse, ml-1m
#
#   model: AutoInt
#   https://recbole.io/docs/user_guide/model/context/autoint.html
#   https://github.com/RUCAIBox/RecBole/blob/v1.1.1/recbole/model/context_aware_recommender/autoint.py
#
#   paper: https://dl.acm.org/doi/10.1145/3357384.3357925
#   paper: https://arxiv.org/abs/1810.11921 


###
# General section
###

    logger_level: 'INFO' # 'WARNING'
    journal_name: 'autoint_distils.csv'    

###
# Training settings section
###

    data_path: './datasets'  # Path to the dataset directory
    dataset: 'amazon_cds'  # ml-100k, Amazon_All_Beauty, steam. Download dataset from: https://github.com/RUCAIBox/RecSysDatasets
    seed: 42
    show_progress: False
    save: True
    reproducibility: True
    epochs: 3  # Number of epochs to train
    learning_rate: 0.01  # Learning rate
    use_gpu: False  # Use GPU for training
    eval_batch_size: 64
    train_batch_size: 64
    stopping_step: 50
    shuffle: True
    repeatable: True
    metrics: [ 'Recall', 'MRR', 'NDCG', 'Hit', 'MAP' ]
    valid_metric: 'MRR@10'    
    benchmark_filename: ['part1', 'part2', 'part3']


###
# Dataset configuration section
###

    USER_ID_FIELD: user_id
    ITEM_ID_FIELD: item_id
    # RATING_FIELD: rating
    # TIME_FIELD: timestamp

    load_col:
        inter: [user_id, item_id, rating, timestamp]
#         user: [user_id, age]
    # user_inter_num_interval: "[450,500)"
#    item_inter_num_interval: "[101,inf)"

    
    # min_user_inter_num: 5
    # max_user_inter_num: 500
    # val_interval:
    #    rating: "[3,inf)"
    #    rating: "[1,3)"
    #    timestamp: "[97830000, inf)"

    threshold:
        rating: 3
    # train_neg_sample_args: ~
    # normalize_all: False


    eval_args:
        mode:
            'valid': 'pop20'
            'test': 'uni100'


    # add_train_shuffle: False
#    add_train_split: [0.5, 0.5]

###
# Model setup section
###

    model: 'AutoInt'  # Model to use;

    embedding_size: 96
    attention_size: 16    #   default = 16
    n_layers: 3 # default = 3  #  NonDynamicallyQuantizableLinear(in_features=16, out_features=16, bias=True)
    num_heads: 2 # default = 2  # embed_dim must be divisible by num_heads
    dropout_probs: [0.2, 0.2, 0.2] # default = [0.2, 0.2, 0.2]
    mlp_hidden_size: [128, 128] # default = [128, 128]

    llm_config:
        embeddings_path: './datasets/llm_embeddings'   
        default_users_emb_file: 'user.pt'
        default_items_emb_file: 'item.pt'
        
        preprocess_from_json:    
            from: 'user_embs.json'
            to: 'user1536.pt'
            by: 'exp_2024_04_11'

        preprocess_random:        
            random: False  # in case if we want generate random embeddings
            embedding_size: 100 # size of embeddings for random embeddings
            random_users_emb_file: 'user.pt'
            random_items_emb_file: 'item.pt'
          
        use_user_emb: True # Do we need loading users emb 
        use_item_emb: False # Do we need loading items emb
        
        users_start_with_1: True  # Add zero string at 0 position of embeddings matrix
        items_start_with_1: True # Add zero string at 0 position of embeddings matrix

        
    distil_loss_weight: 0.5  # [0, 1]
    
###
# Scenario Training setup
###

    scenario:
        [
        # Part 1 Destilation
            {
            'command': 'print',
            'params': '-----Part #1: distilation'
            },
            {
            'command': 'set',
            'params': {'train_part': 0}
            },
            {
            'command': 'init_model',
            'params': None
            },
            {
            'command': 'reduce_dim',
            'params': {
                'file_in': 'user1536.pt',  
                'file_out': 'user32.pt', 
                'dimension': 32, 
                'overwrite': True
                }
            },
            {
            'command': 'wrap_model',
            # Available params: users_emb_file, items_emb_file
            'params': {
                'distil_loss': 'AutoIntUserRMSE', 
                'users_emb_file': 'user32.pt',
                'particular': True 
                }
            },
            {
            'command': 'set_config',
            'params': {'epochs': 2}
            },            
            {
            'command': 'init_trainer',
            'params': None
            },
            {
            'command': 'info',
            'params': 0
            },
#             {
#             'command': 'set_trainable',
#             'params': [['*'], True]  # list of layers should be changed on True/False
#             },
            {
            'command': 'info',
            'params': 0
            },
#             {
#             'command': 'set_outputs',
#             'params': [23]
#             },
            {
            'command': 'set_train_dataset',
            'params': 0 # 0 or 1, as far as there is only two options
            },
#             {
#             'command': 'break'
#             },            
            {
            'command': 'train',
            'params': None
            },
            {
            'command': 'test_eval',
            'params': None
            },
         # Part 2 Learning
            {
            'command': 'print',
            'params': '-----Part #2: fine-tune without distilation'
            },
            {
            'command': 'set',
            'params': {'train_part': 1}
            },         
            {
            'command': 'remove_outputs',
            'params': None
            },
            {
            'command': 'set_config',
            'params': {'epochs': 2}
            },    
            {
            'command': 'init_trainer',
            'params': None
            },
            {
            'command': 'info',
            'params': 0
            },            
            #            {
            #'command': 'set_trainable',
            #'params': [[0, 1, 2, 3], True]  # list of layers should be changed on True/False
            #},
            #{
            #'command': 'set_trainable',
            #'params': [[4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14], False]  # list of layers should be changed on True/False
            #},    
            {
            'command': 'info',
            'params': 0
            },                        
            {
            'command': 'set_train_dataset',
            'params': 0 # 0 or 1, as far as there is only two options
            },
            {
            'command': 'train',
            'params': None
            },
            {
            'command': 'test_eval',
            'params': None
            }            
        ]
